EVALUATION GOAL FOR demo_clinic_beta/patient_questions

Target: clients/demo_clinic_beta/patient_questions/flow_definition.py + services.yaml
Approach: Follow example_simulated_multi_turn_conversations.ipynb pattern
Purpose: Pre-production simulation to validate prompts before deploying

THE WORKFLOW WE ARE TESTING:
- Dial-in flow where patients call Demo Clinic Beta with questions
- Node 1: greeting - bot greets, function: proceed_to_conversation
- Node 2: conversation - bot answers questions, function: end_call
- Uses classifier_llm (Groq) for greeting, main_llm (OpenAI) for conversation

WHAT SUCCESS LOOKS LIKE:
- Simulate 50 conversations with different persona/scenario pairs
- Developer reads each conversation, marks pass/fail
- Developer can override LLM-as-Judge scores
- Developer iterates on prompts in flow_definition.py, re-runs, compares

IMPLEMENTATION RULES:
- Mirror production exactly: use same prompts from flow_definition.py
- No code duplication: import prompts from flow_definition.py, dont copy them
- Ultra lean: only what is needed for this one workflow
- Langfuse integration: dataset.run_experiment() for comparison

DATASET STRUCTURE (per example_simulated_multi_turn_conversations.ipynb):
- Each item: persona + scenario pair
- Simulated user agent plays the persona
- Bot responds using production prompts
- Full conversation stored, scored pass/fail

WHAT WE CHECK:
- Does bot call proceed_to_conversation after greeting?
- Does bot call end_call when caller says goodbye?
- Is response professional, concise, on-topic?
- Does bot identify as Virtual Assistant from Demo Clinic Beta?

LANGFUSE WORKFLOW:
1. langfuse.create_dataset("patient_questions_simulations")
2. Add persona/scenario items
3. dataset.run_experiment() runs simulated conversations
4. LLM-as-Judge scores pass/fail
5. Human reviewer can override in Langfuse UI
6. Change prompts, re-run experiment, compare scores

================================================================================
DIRECTORY STRUCTURE FOR evals/demo_clinic_beta/patient_questions/
================================================================================

evals/demo_clinic_beta/patient_questions/
├── __init__.py              # Already exists - package marker
├── run.py                   # Single file: everything needed to run simulations

That's it. ONE file.

--------------------------------------------------------------------------------
NORTH STAR:
--------------------------------------------------------------------------------

Run `python run.py` from the directory:
1. 10 simulated end-to-end multi-turn conversations
2. Agent calls functions and operates with database JUST LIKE PRODUCTION
3. Developer reviews transcripts in Langfuse, iterates on prompts

--------------------------------------------------------------------------------
WHAT RUNS IN PRODUCTION vs EVAL:
--------------------------------------------------------------------------------

PRODUCTION (full pipeline):
  PatientQuestionsFlow instantiated with:
    - patient_data ✓
    - flow_manager (FlowManager) → orchestrates node transitions
    - main_llm (OpenAI service) → for conversation
    - classifier_llm (Groq service) → for greeting
    - context_aggregator → pushes frames for LLM switching
    - transport → Daily.co
    - pipeline → saves transcripts

  Flow execution:
    1. FlowManager calls create_greeting_node()
    2. LLM receives prompts, may call proceed_to_conversation()
    3. Handler _proceed_to_conversation_handler() returns next node
    4. FlowManager transitions to conversation node
    5. LLM may call end_call()
    6. Handler _end_call_handler() saves transcript, updates DB

EVAL (text simulation):
  PatientQuestionsFlow instantiated with:
    - patient_data ✓ (same as production)
    - flow_manager = MockFlowManager (tracks state, no frames)
    - main_llm = None (we call OpenAI directly)
    - classifier_llm = None (we call OpenAI directly)
    - context_aggregator = MockContextAggregator (no-op frame pushing)
    - transport = None
    - pipeline = MockPipeline (collects transcripts in memory)

  Flow execution:
    1. Runner calls create_greeting_node() → gets ACTUAL prompts
    2. Runner calls OpenAI with prompts + function schemas from node
    3. If LLM calls proceed_to_conversation → Runner calls ACTUAL handler
    4. Handler returns next node → Runner continues with new node
    5. If LLM calls end_call → Runner calls ACTUAL handler
    6. Handler updates DB (real), saves transcript (to memory)

KEY: Function handlers are REAL. They run production code.
     Only infrastructure (frames, transport) is mocked.

--------------------------------------------------------------------------------
run.py STRUCTURE:
--------------------------------------------------------------------------------

# === MOCKS (minimal, just for infrastructure) ===
class MockContextAggregator:
    """No-op frame pushing. Handlers call this but nothing happens."""
    def assistant(self): return self
    async def push_frame(self, frame, direction): pass  # no-op

class MockPipeline:
    """Collects transcripts in memory instead of saving to DB."""
    transcripts = []

class MockFlowManager:
    """Tracks current node. Handlers receive this."""
    pass

# === FLOW RUNNER ===
class FlowRunner:
    """Runs PatientQuestionsFlow without Pipecat pipeline."""

    def __init__(self, patient_data: dict, use_real_db: bool = False):
        from clients.demo_clinic_beta.patient_questions.flow_definition import PatientQuestionsFlow

        self.mock_context = MockContextAggregator()
        self.mock_pipeline = MockPipeline()
        self.mock_flow_manager = MockFlowManager()

        # Instantiate ACTUAL production flow class
        self.flow = PatientQuestionsFlow(
            patient_data=patient_data,
            flow_manager=self.mock_flow_manager,
            main_llm=None,  # We call LLM directly
            classifier_llm=None,
            context_aggregator=self.mock_context,
            transport=None,
            pipeline=self.mock_pipeline if not use_real_db else None,
            organization_id=patient_data.get('organization_id')
        )

        # Start at greeting node
        self.current_node = self.flow.create_greeting_node()
        self.conversation_history = []
        self.function_calls = []
        self.done = False

    def get_current_prompts(self) -> list[dict]:
        """Extract prompts from current node - ACTUAL production prompts."""
        return self.current_node.role_messages + self.current_node.task_messages

    def get_function_schemas(self) -> list[dict]:
        """Extract function definitions from current node."""
        return [
            {
                "name": f.name,
                "description": f.description,
                "parameters": {"type": "object", "properties": f.properties, "required": f.required}
            }
            for f in self.current_node.functions
        ]

    async def process_user_message(self, user_message: str) -> str:
        """Send message to LLM, handle function calls, return response."""
        self.conversation_history.append({"role": "user", "content": user_message})

        # Call LLM with current node's prompts + conversation history
        response = await self._call_llm()

        # If LLM called a function
        if response.function_call:
            func_name = response.function_call.name
            self.function_calls.append(func_name)

            # Find and call the ACTUAL handler from the flow
            handler = self._get_handler(func_name)
            result, next_node = await handler({}, self.mock_flow_manager)

            # Transition to next node (or end if None)
            if next_node is None:
                self.done = True
            else:
                self.current_node = next_node

        self.conversation_history.append({"role": "assistant", "content": response.content})
        return response.content

    def _get_handler(self, func_name: str):
        """Get actual handler function from current node."""
        for f in self.current_node.functions:
            if f.name == func_name:
                return f.handler
        raise ValueError(f"Unknown function: {func_name}")

    async def _call_llm(self):
        """Call OpenAI with current prompts + history + functions."""
        # Uses openai client directly, not Pipecat service
        pass

# === SIMULATED USER ===
def create_simulated_user(persona: str, scenario: str):
    """Creates OpenEvals simulated user agent."""
    from openevals.simulators import create_llm_simulated_user
    return create_llm_simulated_user(
        system=f"You are: {persona}\nSituation: {scenario}\nBe natural, ask questions.",
        model="openai:gpt-4o-mini"
    )

# === CONVERSATION RUNNER ===
async def run_conversation(patient_data: dict, persona: str, scenario: str) -> dict:
    """Run one complete simulated conversation."""
    runner = FlowRunner(patient_data)
    user = create_simulated_user(persona, scenario)

    trajectory = []

    # Greeting (bot speaks first if respond_immediately=True)
    if runner.current_node.respond_immediately:
        bot_response = await runner.process_user_message("")
        trajectory.append({"role": "assistant", "content": bot_response})

    # Conversation loop
    while not runner.done and len(trajectory) < 20:  # max turns safety
        user_msg = user.generate_response(trajectory)
        trajectory.append({"role": "user", "content": user_msg})

        bot_response = await runner.process_user_message(user_msg)
        trajectory.append({"role": "assistant", "content": bot_response})

    return {
        "trajectory": trajectory,
        "function_calls": runner.function_calls,
        "patient_data": patient_data
    }

# === DATASET ===
PERSONAS = [
    "Anxious elderly patient, hard of hearing, asks for repetition",
    "Busy professional, wants quick answers, may interrupt",
    "Confused patient, unsure what they called about",
    "Friendly chatty patient, goes off-topic",
    "Non-native English speaker, simple vocabulary",
]

SCENARIOS = [
    "Calling to ask about appointment availability",
    "Calling to ask about prescription refill process",
    "Calling with billing question",
    "Calling to ask for doctor's callback",
    "Calling but dialed wrong number",
]

# === CLI ===
if __name__ == "__main__":
    # python run.py                    → runs 10 conversations, prints results
    # python run.py --seed             → creates Langfuse dataset
    # python run.py --name baseline    → runs experiment, saves to Langfuse

--------------------------------------------------------------------------------
WHAT'S PRODUCTION CODE vs EVAL CODE:
--------------------------------------------------------------------------------

PRODUCTION (unchanged):
  - flow_definition.py: prompts, nodes, function handlers, DB calls
  - services.yaml: LLM configs

EVAL (new):
  - run.py: FlowRunner + mocks + dataset + CLI

Change flow_definition.py → eval automatically tests new prompts/handlers.

--------------------------------------------------------------------------------
WHAT FUNCTION HANDLERS ACTUALLY DO IN EVAL:
--------------------------------------------------------------------------------

_proceed_to_conversation_handler:
  - Logs "Flow: greeting -> conversation"
  - Returns (None, conversation_node)
  → In eval: RUNS AS-IS. Logs work. Returns next node.

_end_call_handler:
  - Calls save_transcript_to_db(pipeline) → In eval: MockPipeline, no-op
  - Calls db.update_call_status() → In eval: REAL DB call (or skip if no patient_id)
  - Pushes EndTaskFrame → In eval: MockContextAggregator, no-op
  → In eval: DB calls work. Frame pushing is no-op.

The handlers run EXACTLY as production. Infrastructure calls become no-ops.
